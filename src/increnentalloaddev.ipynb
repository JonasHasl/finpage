{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5072182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "# Get the directory of the current script\n",
    "#script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Change the current working directory to the script's directory\n",
    "#os.chdir(script_dir)\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "#import dropbox\n",
    "import io\n",
    "import pandas_datareader.data as web\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from pandas_datareader import wb\n",
    "import matplotlib.dates as mdates\n",
    "import sys \n",
    "sys.version\n",
    "from fredapi import Fred\n",
    "\n",
    "FRED_API_KEY = '29f9bb6865c0b3be320b44a846d539ea'\n",
    "token = 'sl.u.AFjjQNO3MVo7z6zjW1yPvaECsPeQxJSSK-pSyNB_lzRRvhdAf_Yta04g4UhsGWFZ5yidFS81E_c472AdQU_KM4daRjA-eWqHjfBsG32cqClBFVYShrFURRkxooHaMTeA46TkX3147_SeIdYcfJHbfnPVwlk9MY4phWGjJc8zTLt3a0qNlyz-h_kAWYhQUJw2ik3QegCmhAUNW5qn9cTi8ba9HwF-R0aDv_GEcPoajRSvp9C-N59zrCiFuAKbPrvOXKItJGJ_YTY006Lpxo__oNE2kzTMOyIh9qnzDcIioxT0bJnGo7tA4-3to-tmeeILZyh1xMT-75mCDnNN54Ys9B6Bawmfyi5wp2DGvQlGZ5jgD5mux3fGdwezx75spblhHFf-ha5ZGKmzZWlIbGWx7zYwfxN9CDemMEZ1Y5-dQuWiyjTasM5cAr1C_uH9meriN7poaBMJr1wxQIs4Nuw1KTs4nq5Xp7mV2EY0yy6Rkk-ugiyYzcnswBTahsngnvDyJ0D6s_4fmfJRW8Zji3wotpYFzmuWrf2-fG-C8xXGqWXae9qBatS9FmyQNpuOAVP5Jjhp_6GvZz8np6lb-mMbnL_S1ieTKmb_aPFl01vlwfGw44OxY6ys741_MVCf2sl6q3q2HDuIb2P5NEAkDhpBNcMmoOJtRXqBudrhF7Oj9jEb1dZ47ZooFe2Rk21mk00j6MyHW7Ro4wblwSib3P-ZULQn6MwIPP4CTmyZxTfVHosgY9F2t9hmHlW9umQuw7LeC8riA_fw5Hk-PSqUhDio0KMRc-IujybUVXkcwaLXvAqYpBIq1H8T-YlJXNcnLF1rRne9kk8EZaStyY28Vsns8p5sNzHZAQ4mfD9U-hhIS05nlli7p2Go2uJx_QXIRUxCyeCzCHHMlsEHBh548N3evwVkXNDYImMx61lG-h8vjNqKK7VPKSkdSjV2FHzS4VWoGixIMGLgrY-d4PrQTGeiA5lwOsL4i7S4aokNxx8VN-9Ym8FHmkvu3fPvX2FQmLPBe9Pt1-mi66D3Q6j34YKz45NO3cg36gfyFlmKfkkkn8868HaBYyhh7tYVQJ2-iK7wAM7HoBugTLVPo1FPie6tXnn0m8W-B6PhrBps2ebSJHBHru7-HFrd2INlN0Rn7K1bqHvvthlTQoxGV2LbJb3Fk2ks9VnlJzUv7ZYAioRgQPvNfXvUU2xbJXwRfbmGVUXeeYtmCQToEUB7wi4A4OWAWjMhi6rO_w83gk6hGmIeymQ06nLl8SL-RJVSP78yLp5v_jWAvTX2KUJKyBYRXFSTeJ88EvRLw10UR-pF00Vpzo6fLA'\n",
    "\n",
    "fred = Fred(FRED_API_KEY)\n",
    "from IPython.core.pylabtools import figsize\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import requests_html\n",
    "from requests_html import HTMLSession\n",
    "#from requests_html import AsyncHTMLSession\n",
    "from requests import get\n",
    "import itertools\n",
    "import re\n",
    "from scipy import signal\n",
    "from fredapi import Fred\n",
    "import yfinance as yf\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "oldecon = pd.read_csv(r\"C:\\Users\\jonas\\Downloads\\newdatabase\\econW.csv\")\n",
    "max_date = (max(oldecon['Date']))\n",
    "\n",
    "print(max_date)\n",
    "start = datetime.datetime.strptime(max_date, '%Y-%m-%d')\n",
    "\n",
    "#end = datetime(2022, 5, 27)\n",
    "end = datetime.datetime.now()\n",
    "spread = web.DataReader('T10Y2Y', 'fred', start, end)\n",
    "spread.reset_index(inplace=True)\n",
    "spread.rename(columns={'DATE':'Date'}, inplace=True)\n",
    "spread['Date']=pd.to_datetime(spread['Date'])\n",
    "spread.loc[spread['T10Y2Y'].rolling(window=153, min_periods=1).min() < 0, 'Inverted12months'] = 1\n",
    "spread.loc[spread['T10Y2Y'].rolling(window=153, min_periods=1).min() > 0, 'Inverted12months'] = 0\n",
    "    \n",
    "SP = yf.Ticker(\"^GSPC\").history(period='max').reset_index()\n",
    "Stock = pd.DataFrame(SP[['Date', 'Close']]).copy()\n",
    "\n",
    "Stock.reset_index(inplace=True)\n",
    "\n",
    "Stock['Date'] = pd.to_datetime(Stock['Date'])\n",
    "Stock['SP Daily Return'] = pd.to_numeric(Stock['Close'], errors='coerce')\n",
    "Stock['SP Daily Return'] = Stock['Close'].pct_change()\n",
    "Stock.isnull().sum()\n",
    "\n",
    "Stock['Cumulative Returns'] = (1 + Stock['SP Daily Return']).cumprod() - 1\n",
    "Stock.rename(columns={'DATE':'Date'}, inplace=True)\n",
    "completedates = pd.DataFrame(pd.date_range(start=start, end=datetime.datetime.now(), freq='B'), columns=['Date'])\n",
    "\n",
    "release_dates = pd.read_excel('https://alfred.stlouisfed.org/release/downloaddates?rid=10&ff=xls',skiprows= range(1,36))['Release: Consumer Price Index'].copy()\n",
    "\n",
    "release_dates = pd.to_datetime(release_dates)\n",
    "\n",
    "release_dates = pd.DataFrame(release_dates)\n",
    "release_dates.rename(columns={'Release: Consumer Price Index':'Release Date'}, inplace=True)\n",
    "\n",
    "release_dates['Date'] = release_dates['Release Date'] - pd.DateOffset(months=1)\n",
    "release_dates['Date'] = release_dates['Date'].to_numpy().astype('datetime64[M]')\n",
    "release_dates['Date'] = pd.to_datetime(release_dates['Date'])\n",
    "FRED_API_KEY = '29f9bb6865c0b3be320b44a846d539ea'\n",
    "fred = Fred(FRED_API_KEY)\n",
    "\n",
    "cpius = fred.get_series_all_releases('CPIAUCNS', realtime_start=start.date())\n",
    "\n",
    "cpius = cpius.drop('date', axis=1).rename(columns={'realtime_start' : 'Date'}).set_index('Date')\n",
    "cpius = cpius.sort_index()\n",
    "\n",
    "cpius.reset_index(inplace= True)\n",
    "cpius = cpius.drop_duplicates(subset='Date', keep='last')\n",
    "cpius['MoM'] = cpius['value'].pct_change()\n",
    "cpius['YoY'] = cpius['value'].pct_change(periods=12)\n",
    "cpius['RollingMean12'] = cpius['value'].pct_change(periods=12).rolling(12, min_periods=1).mean()\n",
    "cpius['Date'] = pd.to_datetime(cpius['Date']) \n",
    "cpius.rename({'value':'CPIUS'}, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "tradebalance = fred.get_series_all_releases('BOPGSTB', realtime_start=start.date())\n",
    "\n",
    "tradebalance = tradebalance.drop('date', axis=1).rename(columns={'realtime_start' : 'Date'}).set_index('Date')\n",
    "tradebalance = tradebalance.sort_index()\n",
    "\n",
    "tradebalance.reset_index(inplace= True)\n",
    "tradebalance = tradebalance.drop_duplicates(subset='Date', keep='last')\n",
    "tradebalance.sort_values(by='Date', ascending=True, inplace=True)\n",
    "tradebalance['value'] = tradebalance['value'].astype(float)\n",
    "\n",
    "tradebalance['MoM'] = (tradebalance['value'] - tradebalance['value'].shift(1).fillna(0)) / tradebalance['value'].shift(1).abs().replace(0, np.nan)\n",
    "tradebalance['YoY'] = (tradebalance['value'] - tradebalance['value'].shift(12).fillna(0)) / tradebalance['value'].shift(12).abs().replace(0, np.nan)\n",
    "tradebalance['MoM'] = tradebalance['MoM'].replace([np.inf, -np.inf, np.nan], 0)\n",
    "tradebalance['YoY'] = tradebalance['YoY'].replace([np.inf, -np.inf, np.nan], 0)\n",
    "tradebalance['MoM'] = tradebalance['MoM'].replace([float('inf'), float('-inf')], 0)\n",
    "tradebalance['YoY'] = tradebalance['YoY'].replace([float('inf'), float('-inf')], 0)\n",
    "\n",
    "\n",
    "tradebalance['RollingMean12'] = tradebalance['value'].pct_change(periods=12).rolling(12, min_periods=1).mean()\n",
    "tradebalance['Date'] = pd.to_datetime(tradebalance['Date']) \n",
    "tradebalance.rename({'value':'Trade Balance'}, axis=1, inplace=True)\n",
    "tradebalance.rename({'MoM':'Trade Balance MoM'}, axis=1, inplace=True)\n",
    "tradebalance.rename({'YoY':'Trade Balance YoY'}, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    \n",
    "figsize(20, 5)\n",
    "pd.options.display.max_colwidth = 60\n",
    "\n",
    "m2 = fred.get_series_all_releases('M2SL', realtime_start=start.date())\n",
    "m2['date'] = pd.to_datetime(m2['date'])\n",
    "m2['value'] = m2['value'].astype(float)\n",
    "m2['m2_growth'] = m2['value'].pct_change(periods=12)\n",
    "m2['MoMGrowthChange'] = m2['m2_growth'].pct_change() # Theory: Effect on stock market is shown next month\n",
    "meanvalues = pd.DataFrame(m2.groupby(['date'])['value'].mean()).rename(columns={'value':'meanvalue'}).drop_duplicates(subset='meanvalue', keep='first')\n",
    "\n",
    "m2 = m2.merge(meanvalues, on='date', how='left').drop_duplicates(subset = ['date'],keep='first').copy()\n",
    "m2['m2_growth'] = m2['value'].pct_change(periods=12)\n",
    "m2['MoMGrowthChange'] = m2['m2_growth'].pct_change() # Theory: Effect on stock market is shown next month\n",
    "m2.tail(10)\n",
    "m2 = m2.drop('date', axis=1).rename(columns={'realtime_start' : 'Date'}).set_index('Date')\n",
    "\n",
    "m2.reset_index(inplace= True)\n",
    "m2 = m2.drop_duplicates('Date', keep='last')\n",
    "m2['Date'] = pd.to_datetime(m2['Date'])\n",
    "m2.rename(columns={'value':'m2'}, inplace=True)\n",
    "    \n",
    "unemp = fred.get_series_all_releases('UNRATE', realtime_start=start.date())\n",
    "unemp['date'] = pd.to_datetime(unemp['date'])\n",
    "unemp['value'] = unemp['value'].astype(float)\n",
    "\n",
    "unemp['unemp_growth'] = unemp['value'].pct_change()\n",
    "#m2['MoMGrowthChange'] = m2['m2_growth'].pct_change() # Theory: Effect on stock market is shown next month\n",
    "meanvalues = pd.DataFrame(unemp.groupby(['date'])['value'].mean()).rename(columns={'value':'meanvalue'}).drop_duplicates(subset='meanvalue', keep='first')\n",
    "\n",
    "unemp = unemp.merge(meanvalues, on='date', how='left').drop_duplicates(subset = ['date'],keep='first').copy()\n",
    "#m2['m2_growth'] = m2['value'].pct_change(periods=12)\n",
    "#m2['MoMGrowthChange'] = m2['m2_growth'].pct_change() # Theory: Effect on stock market is shown next month\n",
    "unemp.tail(10)\n",
    "unemp = unemp.drop('date', axis=1).rename(columns={'realtime_start' : 'Date'}).set_index('Date')\n",
    "\n",
    "unemp.reset_index(inplace= True)\n",
    "unemp = unemp.drop_duplicates('Date', keep='last')\n",
    "unemp['Date'] = pd.to_datetime(unemp['Date'])\n",
    "unemp.rename(columns={'value':'unemp_rate'}, inplace=True)\n",
    "    \n",
    "    \n",
    "tenyearmin = yf.Ticker(\"^TNX\").history(period='max')\n",
    "\n",
    "#tenyearmin = web.DataReader('DGS10', 'fred', start=start, end=end)\n",
    "tenyearmin = pd.DataFrame(tenyearmin['Close']).copy()\n",
    "\n",
    "tenyearmin.reset_index(inplace=True)\n",
    "\n",
    "tenyearmin['Date'] = pd.to_datetime(tenyearmin['Date'])\n",
    "\n",
    "oneyearmin = yf.Ticker(\"^IRX\").history(period='max')\n",
    "#oneyearmin = web.DataReader('DGS1', 'fred', start=start, end=end)\n",
    "oneyearmin = pd.DataFrame(oneyearmin['Close']).copy()\n",
    "\n",
    "oneyearmin.reset_index(inplace=True)\n",
    "\n",
    "oneyearmin['Date'] = pd.to_datetime(oneyearmin['Date'])\n",
    "\n",
    "yieldmin = oneyearmin.merge(tenyearmin, on=\"Date\")\n",
    "yieldmin['spread'] = yieldmin['Close_y'] - yieldmin['Close_x']\n",
    "\n",
    "yieldmin.rename(columns={'Close_x':'OneYearYield', 'Close_y':'TenYield'}, inplace=True)\n",
    "yieldmin.fillna(0, inplace=True)\n",
    "yieldmin.rename(columns={'DATE':'Date'}, inplace=True)\n",
    "\n",
    "completedates = pd.DataFrame(pd.date_range(start=start, end=datetime.datetime.now(), freq='B'), columns=['Date'])\n",
    "\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import requests_html\n",
    "from requests_html import HTMLSession\n",
    "#from requests_html import AsyncHTMLSession\n",
    "from requests import get\n",
    "import itertools\n",
    "import re\n",
    "from scipy import signal\n",
    "my_url = \"https://www.multpl.com/shiller-pe/table/by-month\"\n",
    "session = HTMLSession()\n",
    "response = session.get(my_url)\n",
    "page_content = response.text\n",
    "soup = bs(page_content, 'html.parser')\n",
    "\n",
    "\n",
    "table = soup.select(\"table\")[0]\n",
    "\n",
    "#actual_values = [link['left'] for link in table]\n",
    "\n",
    "columns = soup.find('th', class_=\"left\")\n",
    "\n",
    "table_rows = table.find_all(\"tr\")\n",
    "l = []\n",
    "for tr in table_rows:\n",
    "    td = tr.find_all('td')\n",
    "    row = [str(tr.get_text()).strip() for tr in td]\n",
    "    l.append(row) \n",
    "#print(table_rows)\n",
    "l=list(itertools.chain(*l))\n",
    "Dates = l[0::2]\n",
    "Values = l[1::2]\n",
    "shillers = pd.DataFrame(columns = [Dates, Values]).T.reset_index()\n",
    "shillers.columns = ['Date', 'Value']\n",
    "shillers['Date'] = pd.to_datetime(shillers['Date'], format='mixed')\n",
    "shillers['Value'] = shillers['Value'].astype(float)\n",
    "\n",
    "shiller = shillers\n",
    "\n",
    "shiller['Date'] = pd.to_datetime(shiller['Date'])\n",
    "shiller.columns = ['Date', 'Shiller_P/E'] \n",
    "\n",
    "composite_confidence = fred.get_series_all_releases('CSCICP03USM665S', realtime_start=start.date()) \n",
    "composite_confidence['date'] = pd.to_datetime(composite_confidence['date'])\n",
    "composite_confidence.drop_duplicates(subset='date', keep='first', inplace=True)\n",
    "composite_confidence.head(20)\n",
    "composite_confidence['adjusted_value'] = composite_confidence['value'].shift(2).astype(float)\n",
    "composite_confidence['composite_confidence_growth_yoy'] = composite_confidence['adjusted_value'].pct_change(periods=12)\n",
    "composite_confidence['MoMConfidence'] = composite_confidence['composite_confidence_growth_yoy'].pct_change()\n",
    "composite_confidence\n",
    "\n",
    "#consumer_credit = consumer_credit.drop('date', axis=1).rename(columns={'realtime_start' : 'Date'}).set_index('Date')\n",
    "\n",
    "#\n",
    "#consumer_credit =consumer_credit.drop_duplicates('Date', keep='last')\n",
    "composite_confidence['date'] = pd.to_datetime(composite_confidence['date'])\n",
    "composite_confidence.rename(columns={'date':'Date'}, inplace=True)\n",
    "\n",
    "composite_confidence = composite_confidence.drop(['realtime_start', 'value'], axis=1).set_index('Date').reset_index()\n",
    "#consumer_credit.reset_index(inplace= True)\n",
    "composite_confidence['RollingComp'] = composite_confidence.adjusted_value.rolling(12).mean()\n",
    "composite_confidence\n",
    "\n",
    "inflation_exp = fred.get_series_all_releases('MICH', realtime_start=start.date()) #Total Consumer Credit\n",
    "inflation_exp['date'] = pd.to_datetime(inflation_exp['date'])\n",
    "inflation_exp.drop_duplicates(subset='date', keep='first', inplace=True)\n",
    "inflation_exp.head(20)\n",
    "inflation_exp['adjusted_value'] = inflation_exp['value'].shift(2).astype(float)\n",
    "inflation_exp['inflation_exp_yoy'] = inflation_exp['adjusted_value'].pct_change(periods=12)\n",
    "inflation_exp['MoMInflationExp'] = inflation_exp['inflation_exp_yoy'].pct_change()\n",
    "inflation_exp\n",
    "\n",
    "#consumer_credit = consumer_credit.drop('date', axis=1).rename(columns={'realtime_start' : 'Date'}).set_index('Date')\n",
    "\n",
    "#consumer_credit.reset_index(inplace= True)\n",
    "#consumer_credit =consumer_credit.drop_duplicates('Date', keep='last')\n",
    "inflation_exp['date'] = pd.to_datetime(inflation_exp['date'])\n",
    "inflation_exp.rename(columns={'date':'Date'}, inplace=True)\n",
    "\n",
    "inflation_exp = inflation_exp.drop(['realtime_start', 'value', 'adjusted_value'], axis=1).set_index('Date').reset_index()\n",
    "inflation_exp['RollingExpInfl'] = inflation_exp.inflation_exp_yoy.rolling(12).mean()\n",
    "inflation_exp\n",
    "\n",
    "consumer_confidence = fred.get_series_all_releases('UMCSENT', realtime_start=start.date()) #Total Consumer Credit\n",
    "consumer_confidence.dropna(inplace=True)\n",
    "consumer_confidence['date'] = pd.to_datetime(consumer_confidence['date'])\n",
    "consumer_confidence.drop_duplicates(subset='date', keep='first', inplace=True)\n",
    "consumer_confidence.head(20)\n",
    "consumer_confidence['adjusted_value'] = consumer_confidence['value'].shift(2).astype(float)\n",
    "consumer_confidence['consumer_conf_yoy'] = consumer_confidence['adjusted_value'].pct_change(periods=12)\n",
    "consumer_confidence['MoMConsumerConf'] = consumer_confidence['consumer_conf_yoy'].pct_change()\n",
    "consumer_confidence\n",
    "\n",
    "#consumer_credit = consumer_credit.drop('date', axis=1).rename(columns={'realtime_start' : 'Date'}).set_index('Date')\n",
    "\n",
    "#consumer_credit.reset_index(inplace= True)\n",
    "#consumer_credit =consumer_credit.drop_duplicates('Date', keep='last')\n",
    "consumer_confidence['date'] = pd.to_datetime(consumer_confidence['date'])\n",
    "consumer_confidence.rename(columns={'date':'Date'}, inplace=True)\n",
    "\n",
    "consumer_confidence = consumer_confidence.drop(['realtime_start', 'value', 'adjusted_value'], axis=1).set_index('Date').reset_index()\n",
    "consumer_confidence['RollingConsConf'] = consumer_confidence.consumer_conf_yoy.rolling(12).mean()\n",
    "consumer_confidence\n",
    "\n",
    "import_end = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = import_end - start_time\n",
    "print(f\"Elapsed time imports: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "import pytz\n",
    "time_zone = 'America/New_York'\n",
    "dfs = [completedates, Stock, cpius, m2, yieldmin, unemp, spread, shiller, consumer_confidence, inflation_exp, composite_confidence, tradebalance]\n",
    "for df in dfs:\n",
    "    #df = df.reset_index()\n",
    "    for col in df.columns:\n",
    "        if col == \"Date\":\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "            # extract just the date component\n",
    "            df[col] = df[col].dt.date\n",
    "            #df[col] = df[col].Date\n",
    "            \n",
    "econ = completedates.merge(Stock, on='Date', how='left').merge(cpius, on='Date', how='left').merge(m2, on='Date', how='left').merge(yieldmin, on='Date', how='left').merge(unemp, on='Date', how='left').merge(spread, on='Date', how='left').merge(shiller, on='Date', how='left').merge(consumer_confidence, on='Date', how='left').merge(inflation_exp, on='Date', how='left').merge(composite_confidence, on='Date', how='left').merge(tradebalance, on='Date', how='left').ffill()\n",
    "#.apply(lambda x : x.iloc[0]).head(32)\n",
    "econ.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "econ['real_yield'] = econ['TenYield'] - (econ['YoY']*100)\n",
    "from scipy import signal\n",
    "\n",
    "econ = econ.dropna(axis=0).copy()\n",
    "\n",
    "econ = econ.drop_duplicates()\n",
    "econ.Date = pd.to_datetime(econ['Date'])\n",
    "\n",
    "econ.set_index('Date', inplace=True)\n",
    "econW = econ\n",
    "\n",
    "econW.rename(columns={'inflation_exp_yoy':'InflationExp', 'consumer_conf_yoy':'ConsumerConfidence'}, inplace=True)\n",
    "econW.sort_values('Date', ascending=True, inplace=True)\n",
    "\n",
    "\n",
    "#econW.to_csv(r'C:\\Users\\jonas\\OneDrive\\Skrivebord\\App\\econW.csv')\n",
    "#econW.to_csv(r'C:\\Users\\jonas\\Dropbox\\econW.csv')\n",
    "\n",
    "#print(max(econW.index))\n",
    "\n",
    "oldecon.set_index('Date', inplace=True)\n",
    "#econW = pd.concat([oldecon, econW], axis=0)\n",
    "\n",
    "#econW.to_csv(r'econWnew.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eedf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "from pandas_datareader import data as web\n",
    "from fredapi import Fred\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "FRED_API_KEY = '29f9bb6865c0b3be320b44a846d539ea'\n",
    "fred = Fred(FRED_API_KEY)\n",
    "\n",
    "# --- UTILITY FUNCTIONS ---\n",
    "\n",
    "def load_existing_data(path):\n",
    "    \"\"\"Load existing economic data.\"\"\"\n",
    "    df = pd.read_csv(path, parse_dates=['Date'])\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    return df\n",
    "\n",
    "def get_new_dates(existing_df):\n",
    "    \"\"\"Get the latest date and define the new start date.\"\"\"\n",
    "    max_date = existing_df['Date'].max()\n",
    "    start = max_date + pd.Timedelta(days=1)\n",
    "    end = datetime.datetime.now()\n",
    "    return start, end\n",
    "\n",
    "def fetch_spread(start, end):\n",
    "    \"\"\"Fetch yield curve spread data.\"\"\"\n",
    "    df = web.DataReader('T10Y2Y', 'fred', start, end).reset_index()\n",
    "    df.rename(columns={'DATE': 'Date'}, inplace=True)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    return df\n",
    "\n",
    "def fetch_sp500():\n",
    "    \"\"\"Fetch S&P 500 historical data.\"\"\"\n",
    "    sp = yf.Ticker(\"^GSPC\").history(period='max').reset_index()\n",
    "    sp = sp[['Date', 'Close']]\n",
    "    sp['Date'] = pd.to_datetime(sp['Date'])\n",
    "    return sp\n",
    "\n",
    "def fetch_fred_series(series_id, start):\n",
    "    \"\"\"Fetch a FRED series as a DataFrame.\"\"\"\n",
    "    df = fred.get_series_all_releases(series_id, realtime_start=start.date())\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    return df\n",
    "\n",
    "# --- LOAD EXISTING DATA ---\n",
    "\n",
    "oldecon_path = r\"C:\\Users\\jonas\\Downloads\\newdatabase\\econW.csv\"\n",
    "oldecon = load_existing_data(oldecon_path)\n",
    "start, end = get_new_dates(oldecon)\n",
    "\n",
    "# --- FETCH NEW DATA (UNTOUCHED) ---\n",
    "\n",
    "spread = fetch_spread(start, end)\n",
    "sp500 = fetch_sp500()\n",
    "cpius = fetch_fred_series('CPIAUCNS', start)\n",
    "m2 = fetch_fred_series('M2SL', start)\n",
    "unemp = fetch_fred_series('UNRATE', start)\n",
    "tradebalance = fetch_fred_series('BOPGSTB', start)\n",
    "# Add more series as needed...\n",
    "\n",
    "# --- MERGE ALL DATA ---\n",
    "\n",
    "# Create a date range for merging\n",
    "completedates = pd.DataFrame(pd.date_range(start=start, end=end, freq='B'), columns=['Date'])\n",
    "\n",
    "# Merge all untouched data\n",
    "dfs = [completedates, sp500, cpius, m2, unemp, spread, tradebalance]\n",
    "for df in dfs:\n",
    "    if 'Date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date']).dt.date\n",
    "\n",
    "econ_new = completedates\n",
    "for df in dfs[1:]:\n",
    "    econ_new = econ_new.merge(df, on='Date', how='left')\n",
    "\n",
    "# --- PERFORM CALCULATIONS AFTER MERGE ---\n",
    "\n",
    "econ_new['SP Daily Return'] = econ_new['Close'].pct_change()\n",
    "econ_new['Cumulative Returns'] = (1 + econ_new['SP Daily Return']).cumprod() - 1\n",
    "# Repeat similar calculations for other columns as needed\n",
    "\n",
    "# --- APPEND TO EXISTING DATA AND SAVE ---\n",
    "\n",
    "econ_combined = pd.concat([oldecon, econ_new], ignore_index=True)\n",
    "econ_combined = econ_combined.drop_duplicates(subset='Date').sort_values('Date')\n",
    "\n",
    "# Save to CSV if desired\n",
    "# econ_combined.to_csv(r'path_to_save.csv', index=False)\n",
    "\n",
    "print(\"Incremental update complete. Data shape:\", econ_combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b5f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODIFIED DATA FETCH FUNCTIONS (RAW DATA ONLY) ---\n",
    "def fetch_spread(start, end):\n",
    "    \"\"\"Returns only raw spread data without calculations\"\"\"\n",
    "    spread = web.DataReader('T10Y2Y', 'fred', start, end).reset_index()\n",
    "    spread.rename(columns={'DATE': 'Date'}, inplace=True)\n",
    "    spread['Date'] = pd.to_datetime(spread['Date'])\n",
    "    return spread[['Date', 'T10Y2Y']]\n",
    "\n",
    "def fetch_confidence_indicators(start):\n",
    "    \"\"\"Returns raw confidence data without transformations\"\"\"\n",
    "    def get_raw_series(series_id, name):\n",
    "        df = fred.get_series_all_releases(series_id, realtime_start=start.date())\n",
    "        df = df.rename(columns={'value': name, 'date': 'Date'})\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        return df[['Date', name]].drop_duplicates('Date')\n",
    "    \n",
    "    return {\n",
    "        'composite': get_raw_series('CSCICP03USM665S', 'CompositeConfidence'),\n",
    "        'inflation_exp': get_raw_series('MICH', 'InflationExpectations'),\n",
    "        'consumer': get_raw_series('UMCSENT', 'ConsumerConfidence')\n",
    "    }\n",
    "\n",
    "# --- UPDATED CALCULATION PHASE ---\n",
    "def calculate_metrics(combined_df):\n",
    "    \"\"\"Perform all calculations on merged historical + new data\"\"\"\n",
    "    # Yield curve calculations\n",
    "    combined_df['Inverted12months'] = (combined_df['T10Y2Y']\n",
    "                                        .rolling(window=153, min_periods=1)\n",
    "                                        .min() < 0).astype(int)\n",
    "    \n",
    "    # Stock market calculations\n",
    "    combined_df['SP Daily Return'] = combined_df['Close'].pct_change()\n",
    "    combined_df['Cumulative Returns'] = (1 + combined_df['SP Daily Return']).cumprod() - 1\n",
    "    \n",
    "    # Confidence indicators calculations\n",
    "    for prefix in ['Composite', 'InflationExpectations', 'Consumer']:\n",
    "        col = f\"{prefix}Confidence\"\n",
    "        if col in combined_df.columns:\n",
    "            combined_df[f'{col}_YoY'] = combined_df[col].pct_change(periods=12)\n",
    "            combined_df[f'{col}_MoM'] = combined_df[col].pct_change()\n",
    "            combined_df[f'{col}_Rolling'] = combined_df[col].rolling(12).mean()\n",
    "    \n",
    "    # Economic calculations\n",
    "    combined_df['real_yield'] = combined_df['TenYield'] - (combined_df['CPIUS'].pct_change(periods=12) * 100)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# --- MODIFIED WORKFLOW ---\n",
    "# 1. Load existing data\n",
    "oldecon = load_existing_data(oldecon_path)\n",
    "\n",
    "# 2. Get new raw data\n",
    "start, end = get_new_dates(oldecon)\n",
    "new_data = fetch_new_data(start, end)  # Includes all fetch functions\n",
    "\n",
    "# 3. Merge with historical data\n",
    "combined = pd.concat([oldecon, new_data], ignore_index=True)\n",
    "combined = combined.drop_duplicates(subset='Date').sort_values('Date')\n",
    "\n",
    "# 4. Perform calculations on complete dataset\n",
    "final_df = calculate_metrics(combined)\n",
    "\n",
    "# 5. Save updated data\n",
    "final_df.to_csv(updated_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d36899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching new data from 2000-12-20 to 2025-05-24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_36528\\2942144903.py:143: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df['SP_Daily_Return'] = df['SP500'].pct_change()\n",
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_36528\\2942144903.py:150: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df[f'{col}_YoY'] = df[col].pct_change(12)\n",
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_36528\\2942144903.py:150: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[f'{col}_YoY'] = df[col].pct_change(12)\n",
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_36528\\2942144903.py:151: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df[f'{col}_MoM'] = df[col].pct_change()\n",
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_36528\\2942144903.py:151: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[f'{col}_MoM'] = df[col].pct_change()\n",
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_36528\\2942144903.py:150: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df[f'{col}_YoY'] = df[col].pct_change(12)\n",
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_36528\\2942144903.py:150: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[f'{col}_YoY'] = df[col].pct_change(12)\n",
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_36528\\2942144903.py:151: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df[f'{col}_MoM'] = df[col].pct_change()\n",
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_36528\\2942144903.py:151: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[f'{col}_MoM'] = df[col].pct_change()\n",
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_36528\\2942144903.py:155: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df['Real_Yield'] = df['T10Y2Y'] - (df['CPI'].pct_change(12) * 100)\n",
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_36528\\2942144903.py:155: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Real_Yield'] = df['T10Y2Y'] - (df['CPI'].pct_change(12) * 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update complete. New shape: (8923, 57)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "from pandas_datareader import data as web\n",
    "from fredapi import Fred\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "FRED_API_KEY = '29f9bb6865c0b3be320b44a846d539ea'\n",
    "fred = Fred(FRED_API_KEY)\n",
    "oldecon_path = r\"C:\\Users\\jonas\\Downloads\\newdatabase\\econW.csv\"\n",
    "updated_path = r\"C:\\Users\\jonas\\Downloads\\newdatabase\\econW_updated.csv\"\n",
    "\n",
    "# --- UTILITY FUNCTIONS ---\n",
    "def load_existing_data(path):\n",
    "    \"\"\"Load existing dataset with date parsing\"\"\"\n",
    "    df = pd.read_csv(path, parse_dates=['Date'])\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    return df.sort_values('Date')\n",
    "\n",
    "def get_new_dates(existing_df):\n",
    "    \"\"\"Determine date range for incremental update\"\"\"\n",
    "    max_date = existing_df['Date'].max()\n",
    "    start = max_date + pd.Timedelta(days=1)\n",
    "    end = datetime.datetime.now()\n",
    "    return start, end\n",
    "\n",
    "def ensure_naive_dates(dfs):\n",
    "    \"\"\"Convert all Date columns to timezone-naive datetime\"\"\"\n",
    "    for df in dfs:\n",
    "        if 'Date' in df.columns:\n",
    "            df['Date'] = pd.to_datetime(df['Date']).dt.tz_localize(None)\n",
    "    return dfs\n",
    "\n",
    "\n",
    "# --- DATA FETCHING FUNCTIONS (RAW DATA ONLY) ---\n",
    "def fetch_spread(start, end):\n",
    "    \"\"\"10Y-2Y Treasury Spread\"\"\"\n",
    "    df = web.DataReader('T10Y2Y', 'fred', start, end).reset_index()\n",
    "    return df.rename(columns={'DATE': 'Date'})[['Date', 'T10Y2Y']]\n",
    "\n",
    "def fetch_sp500():\n",
    "    \"\"\"S&P 500 Historical Prices\"\"\"\n",
    "    sp = yf.Ticker(\"^GSPC\").history(period='max').reset_index()\n",
    "    return sp[['Date', 'Close']].rename(columns={'Close': 'SP500'})\n",
    "\n",
    "def fetch_fred_series(series_id, start, name):\n",
    "    \"\"\"Generic FRED Series Fetcher\"\"\"\n",
    "    df = fred.get_series_all_releases(series_id, realtime_start=start.date())\n",
    "    return (df.rename(columns={'date': 'Date', 'value': name})\n",
    "            .drop('realtime_start', axis=1)\n",
    "            .drop_duplicates('Date'))\n",
    "\n",
    "def fetch_shiller_pe():\n",
    "    \"\"\"Shiller P/E Ratio from multpl.com\"\"\"\n",
    "    session = HTMLSession()\n",
    "    response = session.get(\"https://www.multpl.com/shiller-pe/table/by-month\")\n",
    "    soup = bs(response.text, 'html.parser')\n",
    "    \n",
    "    data = []\n",
    "    for tr in soup.select(\"table\")[0].find_all(\"tr\"):\n",
    "        td = tr.find_all('td')\n",
    "        if len(td) == 2:\n",
    "            data.append([\n",
    "                pd.to_datetime(td[0].get_text().strip(), errors='coerce'),\n",
    "                float(td[1].get_text().strip().replace(',', ''))\n",
    "            ])\n",
    "    \n",
    "    return pd.DataFrame(data, columns=['Date', 'Shiller_PE']).dropna()\n",
    "\n",
    "def fetch_confidence_data(start):\n",
    "    \"\"\"Fetch raw confidence indicators\"\"\"\n",
    "    def fetch_one(series_id, name):\n",
    "        df = fred.get_series_all_releases(series_id, realtime_start=start.date())\n",
    "        return (df.rename(columns={'date': 'Date', 'value': name})\n",
    "                .drop('realtime_start', axis=1)\n",
    "                .drop_duplicates('Date'))\n",
    "    \n",
    "    return {\n",
    "        'composite': fetch_one('CSCICP03USM665S', 'CompositeConfidence'),\n",
    "        'inflation': fetch_one('MICH', 'InflationExpectations'),\n",
    "        'consumer': fetch_one('UMCSENT', 'ConsumerConfidence')\n",
    "    }\n",
    "\n",
    "# --- MAIN WORKFLOW ---\n",
    "def main():\n",
    "    # 1. Load existing data\n",
    "    oldecon = load_existing_data(oldecon_path)\n",
    "    \n",
    "    # 2. Determine date range for update\n",
    "    start, end = get_new_dates(oldecon)\n",
    "    print(f\"Fetching new data from {start.date()} to {end.date()}\")\n",
    "    \n",
    "    # 3. Fetch all new raw data\n",
    "    new_data = {\n",
    "        'spread': fetch_spread(start, end),\n",
    "        'sp500': fetch_sp500(),\n",
    "        'cpi': fetch_fred_series('CPIAUCNS', start, 'CPI'),\n",
    "        'm2': fetch_fred_series('M2SL', start, 'M2'),\n",
    "        'unemployment': fetch_fred_series('UNRATE', start, 'Unemployment'),\n",
    "        'trade_balance': fetch_fred_series('BOPGSTB', start, 'TradeBalance'),\n",
    "        'shiller': fetch_shiller_pe(),\n",
    "        **fetch_confidence_data(start)\n",
    "    }\n",
    "\n",
    "    # 4. Create complete date index\n",
    "    date_range = pd.date_range(start=start, end=end, freq='D')\n",
    "    combined = pd.DataFrame({'Date': date_range})\n",
    "\n",
    "    # 5. Merge all datasets\n",
    "    combined = pd.DataFrame({'Date': date_range})\n",
    "\n",
    "    # Convert all Date columns to naive datetime first\n",
    "    new_data = ensure_naive_dates(list(new_data.values()))  # Convert new data\n",
    "    oldecon = ensure_naive_dates([oldecon])[0]  # Convert existing data\n",
    "\n",
    "    for df in new_data:\n",
    "        if not df.empty:\n",
    "            combined = combined.merge(df, on='Date', how='left')\n",
    "\n",
    "\n",
    "    # 6. Combine with historical data\n",
    "    combined = pd.concat([oldecon, combined], ignore_index=True)\n",
    "    combined = combined.drop_duplicates('Date').sort_values('Date')\n",
    "\n",
    "    # 7. Perform all calculations\n",
    "    combined = calculate_metrics(combined)\n",
    "    \n",
    "    # 8. Save updated data\n",
    "    combined.to_csv(updated_path, index=False)\n",
    "    print(f\"Update complete. New shape: {combined.shape}\")\n",
    "\n",
    "# --- CALCULATION ENGINE ---\n",
    "def calculate_metrics(df):\n",
    "    \"\"\"Post-merge calculations with full historical context\"\"\"\n",
    "    # Yield curve calculations\n",
    "    df['Inverted12months'] = (df['T10Y2Y']\n",
    "                             .rolling(153, min_periods=1)\n",
    "                             .min() < 0).astype(int)\n",
    "    \n",
    "    # Stock market calculations\n",
    "    df['SP_Daily_Return'] = df['SP500'].pct_change()\n",
    "    df['SP_Cumulative_Return'] = (1 + df['SP_Daily_Return']).cumprod() - 1\n",
    "    \n",
    "    # Confidence indicators\n",
    "    for prefix in ['Composite', 'Inflation', 'Consumer']:\n",
    "        col = f\"{prefix}Confidence\"\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_YoY'] = df[col].pct_change(12)\n",
    "            df[f'{col}_MoM'] = df[col].pct_change()\n",
    "            df[f'{col}_Rolling'] = df[col].rolling(12).mean()\n",
    "    \n",
    "    # Economic calculations\n",
    "    df['Real_Yield'] = df['T10Y2Y'] - (df['CPI'].pct_change(12) * 100)\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ca2563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
